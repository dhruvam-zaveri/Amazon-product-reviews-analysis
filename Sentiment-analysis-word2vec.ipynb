{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jc/l_cy7_tj5pn7bp93mtmcrhg80000gn/T/ipykernel_11120/3224798244.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/hrishikesh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /Users/hrishikesh/miniconda3/envs/Dhruvam/lib/python3.11/site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/hrishikesh/miniconda3/envs/Dhruvam/lib/python3.11/site-packages (from bs4) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/hrishikesh/miniconda3/envs/Dhruvam/lib/python3.11/site-packages (from beautifulsoup4->bs4) (2.5)\n",
      "Requirement already satisfied: gensim in /Users/hrishikesh/miniconda3/envs/Dhruvam/lib/python3.11/site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/hrishikesh/miniconda3/envs/Dhruvam/lib/python3.11/site-packages (from gensim) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/hrishikesh/miniconda3/envs/Dhruvam/lib/python3.11/site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/hrishikesh/miniconda3/envs/Dhruvam/lib/python3.11/site-packages (from gensim) (6.4.0)\n",
      "Requirement already satisfied: torch in /Users/hrishikesh/miniconda3/envs/Dhruvam/lib/python3.11/site-packages (2.2.0)\n",
      "Requirement already satisfied: filelock in /Users/hrishikesh/miniconda3/envs/Dhruvam/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/hrishikesh/miniconda3/envs/Dhruvam/lib/python3.11/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/hrishikesh/miniconda3/envs/Dhruvam/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/hrishikesh/miniconda3/envs/Dhruvam/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/hrishikesh/miniconda3/envs/Dhruvam/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/hrishikesh/miniconda3/envs/Dhruvam/lib/python3.11/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/hrishikesh/miniconda3/envs/Dhruvam/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/hrishikesh/miniconda3/envs/Dhruvam/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: nltk in /Users/hrishikesh/miniconda3/envs/Dhruvam/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Users/hrishikesh/miniconda3/envs/Dhruvam/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/hrishikesh/miniconda3/envs/Dhruvam/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/hrishikesh/miniconda3/envs/Dhruvam/lib/python3.11/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /Users/hrishikesh/miniconda3/envs/Dhruvam/lib/python3.11/site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: pandas in /Users/hrishikesh/miniconda3/envs/Dhruvam/lib/python3.11/site-packages (2.2.0)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in /Users/hrishikesh/miniconda3/envs/Dhruvam/lib/python3.11/site-packages (from pandas) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/hrishikesh/miniconda3/envs/Dhruvam/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/hrishikesh/miniconda3/envs/Dhruvam/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/hrishikesh/miniconda3/envs/Dhruvam/lib/python3.11/site-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in /Users/hrishikesh/miniconda3/envs/Dhruvam/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.4.0-1-cp311-cp311-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in /Users/hrishikesh/miniconda3/envs/Dhruvam/lib/python3.11/site-packages (from scikit-learn) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/hrishikesh/miniconda3/envs/Dhruvam/lib/python3.11/site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/hrishikesh/miniconda3/envs/Dhruvam/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Using cached scikit_learn-1.4.0-1-cp311-cp311-macosx_12_0_arm64.whl (10.6 MB)\n",
      "Using cached threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: threadpoolctl, scikit-learn\n",
      "Successfully installed scikit-learn-1.4.0 threadpoolctl-3.2.0\n"
     ]
    }
   ],
   "source": [
    "! pip install bs4 # in case you don't have it installed\n",
    "! pip install gensim\n",
    "! pip install torch\n",
    "! pip install nltk\n",
    "! pip install pandas\n",
    "! pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jc/l_cy7_tj5pn7bp93mtmcrhg80000gn/T/ipykernel_11120/3465206082.py:2: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(url, sep='\\t',on_bad_lines='skip')\n"
     ]
    }
   ],
   "source": [
    "url = \"https://web.archive.org/web/20201127142707if_/https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Office_Products_v1_00.tsv.gz\"\n",
    "data = pd.read_csv(url, sep='\\t',on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1. Data Generation\n",
    "The code performs the following actions on a dataframe that likely contains product review information:\n",
    "\n",
    "- It selects the columns \"star_rating\", \"review_headline\", and \"review_body\" from the dataframe.\n",
    "\n",
    "- It combines \"review_headline\" and \"review_body\" into a single column named \"review\", which compiles the full text of each review.\n",
    "\n",
    "- It transforms the \"star_rating\" column into a numerical format, converting any non-numeric values to NaN to maintain data integrity.\n",
    "\n",
    "- It removes any rows that have missing values to ensure the dataset's quality.\n",
    "\n",
    "The code assigns ratings to one of three classes for sentiment analysis—assigning '1' for positive, '2' for negative, and '3' for neutral sentiments, corresponding to ratings above 3, below 3, and equal to 3, respectively. It then randomly selects 50,000 reviews from each class to balance the dataset, using a fixed seed to ensure reproducibility. Finally, it merges these subsets into a single dataframe, creating a well-distributed dataset crucial for developing a reliable sentiment analysis model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exrtacting the required fields from the dataset\n",
    "data[\"review_body\"] = data[\"review_body\"] + ' ' + data[\"review_headline\"]\n",
    "data = data[[\"review_body\", \"star_rating\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:, 'star_rating'] = pd.to_numeric(data.star_rating, errors=\"coerce\")\n",
    "data = data.dropna()\n",
    "\n",
    "# Check and sample data\n",
    "for rating in range(1, 6):\n",
    "    count = min(50000, data[data[\"star_rating\"] == rating].shape[0])\n",
    "    sampled_data = data[data[\"star_rating\"] == rating].sample(n=count, random_state=42)\n",
    "    if rating == 1:\n",
    "        final_sample = sampled_data\n",
    "    else:\n",
    "        final_sample = pd.concat([final_sample, sampled_data])\n",
    "\n",
    "final_sample.reset_index(drop=True, inplace=True)\n",
    "final_sample[\"sentiment\"] = final_sample.star_rating.apply(lambda x: 1 if x > 3 else 3 if x == 3 else 2)\n",
    "\n",
    "# Save dataset for reuse\n",
    "final_sample.to_csv(\"final_sample.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data, sampled_data, url, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62616</th>\n",
       "      <td>Shipping was great, packaging was solid, envel...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128906</th>\n",
       "      <td>Would have liked it better if it was cheaper f...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154991</th>\n",
       "      <td>cute larger notepad, gave as gift to bus drive...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90705</th>\n",
       "      <td>Item that I purchased is not as good as it was...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200634</th>\n",
       "      <td>It looks so nice in my grey beetle. LOVE IT!!!...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review_body star_rating  \\\n",
       "62616   Shipping was great, packaging was solid, envel...         2.0   \n",
       "128906  Would have liked it better if it was cheaper f...         3.0   \n",
       "154991  cute larger notepad, gave as gift to bus drive...         4.0   \n",
       "90705   Item that I purchased is not as good as it was...         2.0   \n",
       "200634  It looks so nice in my grey beetle. LOVE IT!!!...         5.0   \n",
       "\n",
       "        sentiment  \n",
       "62616           2  \n",
       "128906          3  \n",
       "154991          1  \n",
       "90705           2  \n",
       "200634          1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_sample.sample(n=5, random_state=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Pre-processing\n",
    "\n",
    "The data_cleaning and data_preprocessing functions together undertake a series of steps to prepare the text for analysis:\n",
    "\n",
    "- They start by transforming all text to lowercase and stripping away URLs and HTML tags through regular expressions.\n",
    "- The regular expression library is utilized to remove HTML tags to extract plain text.\n",
    "- Any special characters, including punctuation and symbols, are eliminated via regular expressions to simplify the text.\n",
    "- To standardize the text, common contractions are expanded to their full forms using a predefined dictionary, contraction_dict.\n",
    "- A tailored list of stopwords is generated by omitting negation words like 'no', 'nor', and 'not' from the standard list of English stopwords, maintaining the negation context crucial for sentiment analysis. This list is then used to remove stopwords from the text, reducing noise and emphasizing key phrases relevant to sentiment analysis.\n",
    "- Finally, the function outputs the refined text, which is now ready for further analytical or modeling tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert reviews to lowercase strings\n",
    "final_sample[\"review_body\"] = final_sample[\"review_body\"].str.lower()\n",
    "\n",
    "# to remove HTML tags\n",
    "final_sample[\"review_body\"] = final_sample[\"review_body\"].apply(lambda text: re.sub(r'<.*?>', '', text)  if type(text) == str else '')\n",
    "\n",
    "# to remove URLs\n",
    "final_sample[\"review_body\"] = final_sample[\"review_body\"].apply(lambda text: re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE))\n",
    "\n",
    "# to remove non-alphabetic characters except for aphostophes as they will be removed by expanding the contractions\n",
    "final_sample[\"review_body\"] = final_sample[\"review_body\"].apply(lambda text: re.sub(r'[^a-z\\s\\']', '', text))\n",
    "\n",
    "# to remove extra spaces\n",
    "final_sample[\"review_body\"] = final_sample[\"review_body\"].apply(lambda text: re.sub(r'\\s+', ' ', text).strip())\n",
    "\n",
    "# contractions dictionary for expanding the same. For contractions with multiple expansion I have taken the ones that fit most closely according to me. \n",
    "contractions_dict = {\n",
    "        \"i'm\": \"i am\",\n",
    "        \"you're\": \"you are\",\n",
    "        \"he's\": \"he is\",\n",
    "        \"she's\": \"she is\",\n",
    "        \"they're\": \"they are\",\n",
    "        \"we're\": \"we are\",\n",
    "        \"it's\": \"it is\",\n",
    "        \"that's\": \"that is\",\n",
    "        \"here's\": \"here is\",\n",
    "        \"there's\": \"there is\",\n",
    "        \"who's\": \"who is\",\n",
    "        \"where's\": \"where is\",\n",
    "        \"when's\": \"when is\",\n",
    "        \"why's\": \"why is\",\n",
    "        \"what's\": \"what is\",\n",
    "        \"how's\": \"how is\",\n",
    "        \"everybody's\": \"everybody is\",\n",
    "        \"nobody's\": \"nobody is\",\n",
    "        \"something's\": \"something is\",\n",
    "        \"so's\": \"so is\",\n",
    "        \"i'll\": \"i will\",\n",
    "        \"you'll\": \"you will\",\n",
    "        \"he'll\": \"he will\",\n",
    "        \"she'll\": \"she will\",\n",
    "        \"they'll\": \"they will\",\n",
    "        \"it'll\": \"it will\",\n",
    "        \"we'll\": \"we will\",\n",
    "        \"that'll\": \"that will\",\n",
    "        \"this'll\": \"this will\",\n",
    "        \"these'll\": \"these will\",\n",
    "        \"there'll\": \"there will\",\n",
    "        \"where'll\": \"where will\",\n",
    "        \"who'll\": \"who will\",\n",
    "        \"what'll\": \"what will\",\n",
    "        \"how'll\": \"how will\",\n",
    "        \"i've\": \"i have\",\n",
    "        \"you've\": \"you have\",\n",
    "        \"he's\": \"he has\",\n",
    "        \"she's\": \"she has\",\n",
    "        \"we've\": \"we have\",\n",
    "        \"they've\": \"they have\",\n",
    "        \"should've\": \"should have\",\n",
    "        \"could've\": \"could have\",\n",
    "        \"would've\": \"would have\",\n",
    "        \"might've\": \"might have\",\n",
    "        \"must've\": \"must have\",\n",
    "        \"what've\": \"what have\",\n",
    "        \"what's\": \"what has\",\n",
    "        \"where've\": \"where have\",\n",
    "        \"where's\": \"where has\",\n",
    "        \"there've\": \"there have\",\n",
    "        \"there's\": \"there has\",\n",
    "        \"these've\": \"these have\",\n",
    "        \"who's\": \"who has\",\n",
    "        \"don't\": \"do not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"mustn't\": \"must not\",\n",
    "        \"aren't\": \"are not\",\n",
    "        \"couldn't\": \"could not\",\n",
    "        \"wouldn't\": \"would not\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"isn't\": \"is not\",\n",
    "        \"doesn't\": \"does not\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"hasn't\": \"has not\",\n",
    "        \"hadn't\": \"had not\",\n",
    "        \"haven't\": \"have not\",\n",
    "        \"wasn't\": \"was not\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"weren't\": \"were not\",\n",
    "        \"ain't\": \"am not\",\n",
    "        \"let's\": \"let us\",\n",
    "        \"y'all\": \"you all\",\n",
    "        \"where'd\": \"where did\",\n",
    "        \"how'd\": \"how did\",\n",
    "        \"why'd\": \"why did\",\n",
    "        \"who'd\": \"who did\",\n",
    "        \"when'd\": \"when did\",\n",
    "        \"what'd\": \"what did\",\n",
    "        \"g'day\": \"good day\",\n",
    "        \"ma'am\": \"madam\",\n",
    "        \"o'clock\": \"of the clock\"\n",
    "    }\n",
    "# expanding contractions\n",
    "final_sample[\"review_body\"] = final_sample[\"review_body\"].apply(lambda text: ' '.join([contractions_dict[word] if word in contractions_dict else word for word in text.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del contractions_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hrishikesh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         review_body star_rating  sentiment\n",
      "0  keyboard not sensitive enough takes ages retyp...         1.0          2\n",
      "1                         come buy sony's site price         1.0          2\n",
      "2  well happy save lot money first despite full s...         1.0          2\n",
      "3  not please order replaced lamp tv worked mins ...         1.0          2\n",
      "4  bought new not refurbished one place ipod g do...         1.0          2\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stopwords_en = stopwords.words('english')\n",
    "stopwords_en = list(set(stopwords_en) - set(['no', 'nor','not', 'only', 'very', \"don't\", \"ain't\", \"aren't\", \"couldn't\", \"didn't\", \"doesn't\", \"hadn't\", \"hasn't\", \"might't\",\"musn't\", \"isn't\", \"needn't\", \"shan't\", \"shouldn't\", \"wasn't\", \"weren't\", \"wont't\", \"wouldn't\"]))\n",
    "\n",
    "# removing stop words\n",
    "# final_sample[\"review_body\"] = final_sample[\"review_body\"].apply(lambda text: ' '.join([word if word not in stopwords_en else '' for word in text.split()]))\n",
    "final_sample[\"review_body\"] = final_sample[\"review_body\"].apply( lambda x : ' '.join([i for i in x.split() if i not in (stopwords_en)]))\n",
    "\n",
    "print(final_sample.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del stopwords, stopwords_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform lemmatization  \n",
    "\n",
    "The lemmatization process reduces words to their base or root form, aiding in the consolidation of vocabulary variations and enhancing the efficiency of NLP tasks. For instance, \"running\" transforms into \"run\", and \"better\" becomes \"good\". A crucial step in accurate lemmatization is the use of correct part-of-speech (POS) tagging, as the lemma of a word can vary based on its role in a sentence, such as whether it's used as a verb, noun, adjective, or adverb. The code employs NLTK's pos_tag function for POS tagging, which categorizes each word by its grammatical function. To align NLTK's POS tags with those recognized by WordNet, a mapping dictionary (tag_map) is used, ensuring the lemmatizer correctly interprets the grammatical context and applies the suitable lemma for each word. Consequently, the lemmatization step outputs a text string where words are converted to their lemmatized forms, effectively streamlining the input text for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/hrishikesh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/hrishikesh/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import defaultdict\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# creating a pos tag map for proper lemmatization\n",
    "pos_tag_map = defaultdict(lambda : wn.NOUN)\n",
    "pos_tag_map['J'] = wn.ADJ\n",
    "pos_tag_map['V'] = wn.VERB\n",
    "pos_tag_map['R'] = wn.ADV\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# for each review: we would contextually tokenize the sentence using word_tokenize, this will give us the word token and it's pos_tag. This tag will be mapped with pos_tag_map to get the appropriate wordnet pos_tag that will be feed into the lemmatizer.\n",
    "final_sample[\"review_lemmatize\"] = final_sample[\"review_body\"].apply(lambda text: ' '.join([lemmatizer.lemmatize(word_token, pos_tag_map[word_pos_tag[0]])\n",
    " for word_token, word_pos_tag in pos_tag(word_tokenize(text))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sample = final_sample.drop([\"review_body\", \"star_rating\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del lemmatizer, pos_tag_map, wn\n",
    "final_sample.to_csv(\"final_sample_lemmatized.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset\n",
    "\n",
    "A duplicate of the dataset, which has undergone lemmatization, is generated to distinctively segregate the dataset for ternary classification from that of binary classification. This process involves splitting the data, where 80% is allocated for training and the remaining 20% for testing purposes. The separation facilitates the differentiation between the two types of classification tasks, ensuring that the nuanced distinctions in sentiment analysis—capturing positive, negative, and neutral sentiments—are adequately addressed for ternary classification, while also maintaining a dataset specifically tailored for binary classification scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "final_copy = copy.deepcopy(final_sample)\n",
    "final_copy.drop(final_copy[final_copy.sentiment == 3].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data_binary, test_data_binary = train_test_split(final_copy, test_size=0.2, random_state=42)\n",
    "train_data_ternary, test_data_ternary = train_test_split(final_sample, test_size=0.2, random_state=42)\n",
    "\n",
    "train_labels_binary = train_data_binary[\"sentiment\"]\n",
    "test_labels_binary = test_data_binary[\"sentiment\"]\n",
    "\n",
    "train_labels_ternary = train_data_ternary[\"sentiment\"]\n",
    "test_labels_ternary = test_data_ternary[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Load the pretrained “word2vec-google-news-300” Word2Vec model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (summer - hot) + cold = winter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('winter', 0.6970129609107971)]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    semantic_sim = wv.most_similar(positive=['summer', 'cold'], negative=['hot'],topn=1)\n",
    "    print(semantic_sim)\n",
    "except KeyError:\n",
    "    print(\"Keys \\'summer\\', \\'cold\\' or \\'hot\\' not present in word2vec-google-news-300 model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (father - man) + woman = mother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('mother', 0.8462507128715515)]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    semantic_sim = wv.most_similar(positive=['father', 'woman'], negative=['man'], topn=1)\n",
    "    print(semantic_sim)\n",
    "except KeyError:\n",
    "    print(\"Keys \\'father\\', \\'woman\\' or \\'man\\' not present in word2vec-google-news-300 model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Train a Word2Vec model using my dataset.\n",
    "\n",
    "Training a Word2Vec model with specific parameters optimizes its performance for particular natural language processing tasks. By setting the vector size to 300, you're determining the dimensionality of the word vectors, capturing a wide array of linguistic information. A window size of 11 allows the model to consider an extended context around each word, enhancing its ability to understand word meanings based on surrounding words. The minimum count of 10 filters out infrequent words, focusing the model's learning on more relevant vocabulary. Additionally, specifying the number of workers increases computational efficiency by paralleling the training process across multiple cores. This approach not only accelerates the model's training time but also leverages computational resources more effectively, leading to quicker iterations and refinements of the Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess \n",
    "\n",
    "my_model = Word2Vec(sentences=train_data_binary.review_lemmatize.apply(lambda x: simple_preprocess(x)), vector_size=300, window=11, min_count=10, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('winter', 0.7141301035881042)]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    semantic_sim = my_model.wv.most_similar(positive=['summer', 'cold'], negative=['hot'],topn=1)\n",
    "    print(semantic_sim)\n",
    "except KeyError:\n",
    "    print(\"Keys \\'summer\\', \\'cold\\' or \\'hot\\' not present in my model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('sister', 0.682122528553009)]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    semantic_sim = my_model.wv.most_similar(positive=['father', 'woman'], negative=['man'], topn=1)\n",
    "    print(semantic_sim)\n",
    "except KeyError:\n",
    "    print(\"Keys \\'father\\', \\'woman\\' or \\'man\\' not present in my model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you conclude from comparing vectors generated by yourself and the pretrained model? Which of the Word2Vec models seems to encode semantic similarities between words better?\n",
    "\n",
    "The superior performance of the pretrained model in identifying semantic similarities, as demonstrated by the similarity probabilities of the provided examples, can largely be attributed to its extensive vocabulary. This comprehensive vocabulary equips the pretrained model with a broader linguistic understanding, enabling it to discern semantic nuances more effectively. The depth and breadth of the training data behind the pretrained model allow it to capture a wide range of linguistic relationships and subtleties, which might not be as pronounced in a model trained on a more limited dataset. Consequently, the pretrained model's ability to accurately predict semantic similarities surpasses that of the custom-trained model, reflecting the advantages of leveraging large-scale, diverse datasets for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Simple models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `averaged_word_vectorizer` computes the average Word2Vec embedding for each sentence in a given corpus. It initializes an array `all_embeddings` to store the averaged embeddings for the entire corpus, with the shape determined by the number of sentences in the corpus and the specified number of features (`num_features`). For each sentence, it tokenizes the sentence into words, retrieves the Word2Vec vector for each word present in the model's vocabulary, and calculates the mean of these vectors to obtain a single averaged embedding. If a sentence does not contain any words present in the model, a zero vector is used as its embedding. This function returns a 2D numpy array where each row corresponds to the averaged embedding of a sentence from the corpus, effectively condensing the semantic information of each sentence into a fixed-size vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    all_embeddings = np.zeros((corpus.shape[0], num_features))\n",
    "    i = 0\n",
    "    for sentence in corpus:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        vectors = [model[word] for word in tokens if word in model]\n",
    "        \n",
    "        if vectors:\n",
    "            embedding = np.mean(vectors, axis=0, dtype=np.float32)\n",
    "        else:\n",
    "            embedding = np.zeros(model.vector_size, dtype=np.float32)\n",
    "        all_embeddings[i] = embedding\n",
    "        i += 1\n",
    "        \n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create averaged word vector features\n",
    "pretrained_word_embedding = averaged_word_vectorizer(train_data_binary['review_lemmatize'], wv, 300)\n",
    "my_word_embedding= averaged_word_vectorizer(train_data_binary['review_lemmatize'], my_model.wv, 300)\n",
    "\n",
    "pretrained_test_embedding = averaged_word_vectorizer(test_data_binary['review_lemmatize'], wv, 300)\n",
    "my_test_embedding = averaged_word_vectorizer(test_data_binary['review_lemmatize'], my_model.wv, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train_test_model` function encapsulates the process of training a given machine learning model on a dataset and evaluating its performance on a test set. The function begins by fitting the model using the provided `train_data` and `train_labels`. After training, it uses the model to predict the outcomes on the `test_data`. It then calculates four key metrics to assess the model's performance: accuracy, precision, recall, and F1 score, using the true labels from `test_labels` and the predicted labels. Finally, it prints these metrics to provide insights into how well the model performs, specifically in terms of its overall correctness (accuracy), its ability to identify positive instances (precision), its effectiveness in identifying actual positives (recall), and a combined measure of precision and recall (F1 score). This function offers a comprehensive view of a model's predictive quality on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def train_test_model(model, train_data, test_data, train_labels, test_labels):\n",
    "    model.fit(train_data, train_labels)\n",
    "    predictions = model.predict(test_data)\n",
    "\n",
    "    accuracy_test = accuracy_score(test_labels, predictions)\n",
    "    precision_test = precision_score(test_labels, predictions)\n",
    "    recall_test = recall_score(test_labels, predictions)\n",
    "    f1_test = f1_score(test_labels, predictions)\n",
    "\n",
    "    print(\"Testing metics:\")\n",
    "    print(\"Accuracy:\", accuracy_test)\n",
    "    print(\"Precision:\", precision_test)\n",
    "    print(\"Recall:\", recall_test)\n",
    "    print(\"F1 Score:\", f1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained model\n",
      "Testing metics:\n",
      "Accuracy: 0.8139\n",
      "Precision: 0.766131423971529\n",
      "Recall: 0.9038336582196231\n",
      "F1 Score: 0.8293052052281586\n",
      "\n",
      "Model trained by me\n",
      "Testing metics:\n",
      "Accuracy: 0.8218\n",
      "Precision: 0.7574873045703546\n",
      "Recall: 0.946868595991403\n",
      "F1 Score: 0.8416562999822286\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "# google model\n",
    "perceptron = Perceptron(max_iter=1000, alpha=1e-4, penalty='elasticnet', tol=1e-5, random_state=42)\n",
    "print(\"Pretrained model\")\n",
    "train_test_model(perceptron, pretrained_word_embedding, pretrained_test_embedding, train_labels_binary, test_labels_binary)\n",
    "\n",
    "# my model\n",
    "print(\"\\nModel trained by me\")\n",
    "train_test_model(perceptron, my_word_embedding, my_test_embedding, train_labels_binary, test_labels_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron with TF-IDF\n",
    "The code initializes a `TfidfVectorizer` to extract up to 5000 unigram and bigram features from the 'review_lemmatize' column of `final_copy`, transforming the text into a matrix of TF-IDF values. It then constructs a DataFrame, `feature_df`, with these features as columns, facilitating the analysis of the most relevant words and word pairs across the corpus. This approach converts textual data into a numerical format that's ready for machine learning applications, emphasizing words unique to specific documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(final_copy['review_lemmatize'])\n",
    "\n",
    "# to get the features that the vectorizer selected \n",
    "feature_df = pd.DataFrame(tfidf_features.toarray(), columns=tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spliting data for TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = feature_df\n",
    "y = final_copy[\"sentiment\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF\n",
      "Testing metics:\n",
      "Accuracy: 0.912625\n",
      "Precision: 0.9057800058979653\n",
      "Recall: 0.9211276053381316\n",
      "F1 Score: 0.9133893390825961\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "# Initialize the Perceptron model\n",
    "perceptron = Perceptron()\n",
    "print(\"TF-IDF\")\n",
    "train_test_model(perceptron, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained model\n",
      "Testing metics:\n",
      "Accuracy: 0.862375\n",
      "Precision: 0.8768319301527908\n",
      "Recall: 0.8433048433048433\n",
      "F1 Score: 0.8597416494687763\n",
      "\n",
      "Model trained by me\n",
      "Testing metics:\n",
      "Accuracy: 0.898825\n",
      "Precision: 0.9085184805979318\n",
      "Recall: 0.8870395361623432\n",
      "F1 Score: 0.897650539945879\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# google model\n",
    "svm = LinearSVC(dual='auto')\n",
    "print(\"Pretrained model\")\n",
    "train_test_model(svm, pretrained_word_embedding, pretrained_test_embedding, train_labels_binary, test_labels_binary)\n",
    "\n",
    "# my model\n",
    "print(\"\\nModel trained by me\")\n",
    "train_test_model(svm, my_word_embedding, my_test_embedding, train_labels_binary, test_labels_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF\n",
      "Testing metics:\n",
      "Accuracy: 0.9295\n",
      "Precision: 0.9308166641600241\n",
      "Recall: 0.9280251911830859\n",
      "F1 Score: 0.9294188316564048\n"
     ]
    }
   ],
   "source": [
    "print(\"TF-IDF\")\n",
    "train_test_model(svm, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "del feature_df, perceptron, svm, X_train, X_test, y_train, y_test, tfidf_features, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you conclude from comparing performances for the models trained using the three different feature types (TF-IDF, pretrained Word2Vec, your trained Word2Vec)?\n",
    "\n",
    "The comparison of model performances across three different feature types—TF-IDF, pretrained Word2Vec, and a custom-trained Word2Vec—reveals a nuanced understanding of feature effectiveness in sentiment analysis tasks. The highest accuracy is achieved by the SVM model utilizing TF-IDF features, followed closely by the perceptron with TF-IDF, indicating that TF-IDF, with its ability to capture the importance of words within documents, is particularly effective for these classification tasks. The SVM model with embeddings from the custom-trained Word2Vec model outperforms the SVM with embeddings from the pretrained Word2Vec model, suggesting that domain-specific training of word embeddings can lead to better performance than using a generic, pretrained model. However, both fall short of the TF-IDF results, highlighting the challenge of capturing semantic nuances solely through embeddings. Lastly, the perceptron model with embeddings from the pretrained Word2Vec model shows the least accuracy, underscoring the potential limitations of simpler linear models in leveraging deep semantic features extracted from word embeddings. This comparison underscores the importance of feature selection and model complexity in the effectiveness of sentiment analysis tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data for FFNN\n",
    "\n",
    "The code snippet remaps the sentiment labels in two datasets, `final_sample` and `final_copy`, from their original values of 1, 2, or 3 to 0, 1, or 2, respectively. This is achieved by applying a mapping function that substitutes each sentiment value according to the defined dictionary `map`. This adjustment ensures the sentiment labels align with a zero-based indexing system, which is a common requirement for machine learning models, particularly those developed in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model will need labels/classes to be 0, 1, or 2 instead of 1, 2, or 3\n",
    "map = {1: 0, 2: 1, 3: 2}\n",
    "final_sample.sentiment = final_sample.sentiment.apply(lambda x: map[x])\n",
    "final_copy.sentiment = final_copy.sentiment.apply(lambda x: map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_binary, test_data_binary = train_test_split(final_copy, test_size=0.2, random_state=42)\n",
    "train_data_ternary, test_data_ternary = train_test_split(final_sample, test_size=0.2, random_state=42)\n",
    "\n",
    "train_labels_binary = train_data_binary[\"sentiment\"]\n",
    "test_labels_binary = test_data_binary[\"sentiment\"]\n",
    "\n",
    "train_labels_ternary = train_data_ternary[\"sentiment\"]\n",
    "test_labels_ternary = test_data_ternary[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create averaged word vector features\n",
    "pretrained_word_embedding = averaged_word_vectorizer(train_data_binary['review_lemmatize'], wv, 300)\n",
    "my_word_embedding= averaged_word_vectorizer(train_data_binary['review_lemmatize'], my_model.wv, 300)\n",
    "\n",
    "pretrained_test_embedding = averaged_word_vectorizer(test_data_binary['review_lemmatize'], wv, 300)\n",
    "my_test_embedding = averaged_word_vectorizer(test_data_binary['review_lemmatize'], my_model.wv, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code outlines the definition of a `SentimentClassifier` class, a simple feedforward neural network designed for sentiment analysis tasks, using PyTorch's `nn.Module`. This classifier includes the following components:\n",
    "\n",
    "- **Loss Function (`criterion`)**: It employs `nn.CrossEntropyLoss`, a common choice for classification tasks, which combines softmax activation and cross-entropy loss in a single class. This loss function is well-suited for multi-class classification problems.\n",
    "\n",
    "- **Network Architecture**:\n",
    "  - The constructor (`__init__`) receives `input_dim` and `output_dim` as parameters to set up the network dimensions. `input_dim` refers to the size of the input features, and `output_dim` corresponds to the number of classes for the classification task.\n",
    "  - The network comprises three fully connected (`nn.Linear`) layers. The first two linear layers map the input dimensions to 50 and then from 50 to 10. Each of these layers is followed by a ReLU activation function (`nn.ReLU`), introducing non-linearity to the model and enabling it to learn complex patterns.\n",
    "  - The final linear layer (`fc3`) transforms the intermediate representation to the output size specified by `output_dim`, which should match the number of target classes in the sentiment analysis task.\n",
    "\n",
    "- **Forward Pass (`forward` method)**: Defines how the input `x` flows through the network:\n",
    "  - The input is first passed through the `fc1` layer, then activated by ReLU.\n",
    "  - The activated output is fed into the `fc2` layer, followed by another ReLU activation.\n",
    "  - Finally, the output of `fc2` is passed through `fc3` to produce the final output of the model. Note that there is no activation function after `fc3` since `nn.CrossEntropyLoss` expects raw scores (logits) to compute the loss; it applies the softmax function internally.\n",
    "\n",
    "This architecture allows the `SentimentClassifier` to learn from textual feature inputs (like TF-IDF or word embeddings) and predict sentiment labels. The design is straightforward, making it a good starting point for sentiment analysis tasks while remaining flexible enough for further enhancements or adjustments based on specific requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the neural network architecture\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        self.fc3 = nn.Linear(10, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train_model` and `evaluate_model` functions are designed for training and evaluating a multi-layer perceptron (MLP) classifier using PyTorch. It accepts training and testing datasets with their respective labels and parameters defining the model's architecture and training configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model\n",
    "def train_model(model=None, optimizer=None, train_data=None, train_labels=None, epochs=10, batch_size=64):\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    train_features_tensor = torch.tensor(train_data).float()\n",
    "    train_labels_tensor = torch.tensor(train_labels.to_numpy()).long()\n",
    "\n",
    "    # Create TensorDatasets and DataLoaders\n",
    "    train_dataset = TensorDataset(train_features_tensor, train_labels_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        losses = []\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Loss in epoch {epoch}: {sum(losses)/len(losses)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the training phase, the model's accuracy is assessed on the test dataset. The function outputs the loss per epoch during training and the final accuracy on test data, recording these metrics in a `accuracies` dict for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model\n",
    "def evaluate_model(model, test_data=None, test_labels=None, batch_size=64):\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    test_features_tensor = torch.tensor(test_data).float()\n",
    "    test_labels_tensor = torch.tensor(test_labels.to_numpy()).long()\n",
    "    \n",
    "    # Create TensorDatasets and DataLoaders\n",
    "    test_dataset = TensorDataset(test_features_tensor, test_labels_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            # print(torch.max(outputs.data, 1))\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            # print(_, predicted.size())\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    testing_accuracy = correct / total\n",
    "    \n",
    "    print(f'Binary classification accuracy: {testing_accuracy}')\n",
    "    return testing_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_binary = train_labels_binary.reset_index(drop=True)\n",
    "train_labels_ternary = train_labels_ternary.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFNN on binary data with pretained model's features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in epoch 0: 0.3489203453063965\n",
      "Loss in epoch 1: 0.2997763454914093\n",
      "Loss in epoch 2: 0.2846796214580536\n",
      "Loss in epoch 3: 0.27580681443214417\n",
      "Loss in epoch 4: 0.26867684721946716\n",
      "Loss in epoch 5: 0.2633201777935028\n",
      "Loss in epoch 6: 0.2589226961135864\n",
      "Loss in epoch 7: 0.25452709197998047\n",
      "Loss in epoch 8: 0.2508869767189026\n",
      "Loss in epoch 9: 0.2482881397008896\n",
      "Loss in epoch 10: 0.24546802043914795\n",
      "Loss in epoch 11: 0.24271850287914276\n",
      "Loss in epoch 12: 0.24043893814086914\n",
      "Loss in epoch 13: 0.23691962659358978\n",
      "Loss in epoch 14: 0.23528271913528442\n",
      "Loss in epoch 15: 0.2326139211654663\n",
      "Loss in epoch 16: 0.23021991550922394\n",
      "Loss in epoch 17: 0.22890351712703705\n",
      "Loss in epoch 18: 0.22681310772895813\n",
      "Loss in epoch 19: 0.2252882868051529\n",
      "Loss in epoch 20: 0.22352442145347595\n",
      "Loss in epoch 21: 0.2220490425825119\n",
      "Loss in epoch 22: 0.22070713341236115\n",
      "Loss in epoch 23: 0.219153493642807\n",
      "Loss in epoch 24: 0.21718749403953552\n",
      "Loss in epoch 25: 0.21598368883132935\n",
      "Loss in epoch 26: 0.21447589993476868\n",
      "Loss in epoch 27: 0.21347106993198395\n",
      "Loss in epoch 28: 0.21216249465942383\n",
      "Loss in epoch 29: 0.21156299114227295\n",
      "Loss in epoch 30: 0.21003076434135437\n",
      "Loss in epoch 31: 0.2091931700706482\n",
      "Loss in epoch 32: 0.2082393616437912\n",
      "Loss in epoch 33: 0.20742563903331757\n",
      "Loss in epoch 34: 0.20607534050941467\n",
      "Loss in epoch 35: 0.20506718754768372\n",
      "Loss in epoch 36: 0.20399291813373566\n",
      "Loss in epoch 37: 0.20301681756973267\n",
      "Loss in epoch 38: 0.20271717011928558\n",
      "Loss in epoch 39: 0.2017800360918045\n",
      "Loss in epoch 40: 0.20073047280311584\n",
      "Loss in epoch 41: 0.20045709609985352\n",
      "Loss in epoch 42: 0.19977566599845886\n",
      "Loss in epoch 43: 0.1983211189508438\n",
      "Loss in epoch 44: 0.19774378836154938\n",
      "Loss in epoch 45: 0.1969926953315735\n",
      "Loss in epoch 46: 0.1967916190624237\n",
      "Loss in epoch 47: 0.19552001357078552\n",
      "Loss in epoch 48: 0.1953875571489334\n",
      "Loss in epoch 49: 0.19458305835723877\n",
      "Binary classification accuracy: 0.8864\n"
     ]
    }
   ],
   "source": [
    "# Binary classification model\n",
    "binary_model = SentimentClassifier(input_dim=300, output_dim=2) # for binary classification\n",
    "binary_optimizer = optim.Adam(binary_model.parameters())\n",
    "\n",
    "train_model(binary_model, optimizer=binary_optimizer, train_data=pretrained_word_embedding, train_labels=train_labels_binary, epochs=50)\n",
    "\n",
    "# Evaluate the binary model\n",
    "accuracies[\"bin-pre-avg\"] = evaluate_model(binary_model, test_data=pretrained_test_embedding, test_labels=test_labels_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFNN on binary data with my model's features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in epoch 0: 0.2577497065067291\n",
      "Loss in epoch 1: 0.23220810294151306\n",
      "Loss in epoch 2: 0.22442063689231873\n",
      "Loss in epoch 3: 0.21897192299365997\n",
      "Loss in epoch 4: 0.21462886035442352\n",
      "Loss in epoch 5: 0.21122637391090393\n",
      "Loss in epoch 6: 0.2083522528409958\n",
      "Loss in epoch 7: 0.20586560666561127\n",
      "Loss in epoch 8: 0.2031925469636917\n",
      "Loss in epoch 9: 0.20129887759685516\n",
      "Loss in epoch 10: 0.19922475516796112\n",
      "Loss in epoch 11: 0.19748328626155853\n",
      "Loss in epoch 12: 0.19526530802249908\n",
      "Loss in epoch 13: 0.1941741406917572\n",
      "Loss in epoch 14: 0.1925799697637558\n",
      "Loss in epoch 15: 0.19123531877994537\n",
      "Loss in epoch 16: 0.1895778775215149\n",
      "Loss in epoch 17: 0.18833012878894806\n",
      "Loss in epoch 18: 0.186847984790802\n",
      "Loss in epoch 19: 0.18609893321990967\n",
      "Loss in epoch 20: 0.1846332997083664\n",
      "Loss in epoch 21: 0.18396411836147308\n",
      "Loss in epoch 22: 0.18260790407657623\n",
      "Loss in epoch 23: 0.18186454474925995\n",
      "Loss in epoch 24: 0.18091736733913422\n",
      "Loss in epoch 25: 0.18012218177318573\n",
      "Loss in epoch 26: 0.17903977632522583\n",
      "Loss in epoch 27: 0.1786881685256958\n",
      "Loss in epoch 28: 0.17759591341018677\n",
      "Loss in epoch 29: 0.17720648646354675\n",
      "Loss in epoch 30: 0.1759456992149353\n",
      "Loss in epoch 31: 0.17529331147670746\n",
      "Loss in epoch 32: 0.1745634377002716\n",
      "Loss in epoch 33: 0.17389900982379913\n",
      "Loss in epoch 34: 0.17371703684329987\n",
      "Loss in epoch 35: 0.17270348966121674\n",
      "Loss in epoch 36: 0.17206279933452606\n",
      "Loss in epoch 37: 0.17188064754009247\n",
      "Loss in epoch 38: 0.17115896940231323\n",
      "Loss in epoch 39: 0.17063666880130768\n",
      "Loss in epoch 40: 0.1700039654970169\n",
      "Loss in epoch 41: 0.16967801749706268\n",
      "Loss in epoch 42: 0.1688714325428009\n",
      "Loss in epoch 43: 0.1684526801109314\n",
      "Loss in epoch 44: 0.1681961864233017\n",
      "Loss in epoch 45: 0.1674083173274994\n",
      "Loss in epoch 46: 0.1669217348098755\n",
      "Loss in epoch 47: 0.16697847843170166\n",
      "Loss in epoch 48: 0.16621355712413788\n",
      "Loss in epoch 49: 0.16576535999774933\n",
      "Binary classification accuracy: 0.907275\n"
     ]
    }
   ],
   "source": [
    "# Binary classification model\n",
    "binary_model = SentimentClassifier(input_dim=300, output_dim=2) # for binary classification\n",
    "binary_optimizer = optim.Adam(binary_model.parameters())\n",
    "\n",
    "train_model(binary_model, optimizer=binary_optimizer, train_data=my_word_embedding, train_labels=train_labels_binary, epochs=50)\n",
    "\n",
    "# Evaluate the binary model\n",
    "accuracies[\"bin-my-avg\"] = evaluate_model(binary_model, test_data=my_test_embedding, test_labels=test_labels_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFNN on ternary data with pretained model's features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create averaged word vector features\n",
    "pretrained_word_embedding = averaged_word_vectorizer(train_data_ternary['review_lemmatize'], wv, 300)\n",
    "my_word_embedding= averaged_word_vectorizer(train_data_ternary['review_lemmatize'], my_model.wv, 300)\n",
    "\n",
    "pretrained_test_embedding = averaged_word_vectorizer(test_data_ternary['review_lemmatize'], wv, 300)\n",
    "my_test_embedding = averaged_word_vectorizer(test_data_ternary['review_lemmatize'], my_model.wv, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Re-trianing word2vec model on ternary data\n",
    "my_model = Word2Vec(sentences=train_data_ternary.review_lemmatize.apply(lambda x: simple_preprocess(x)), vector_size=300, window=11, min_count=10, workers=4, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in epoch 0: 0.7192530632019043\n",
      "Loss in epoch 1: 0.6450977921485901\n",
      "Loss in epoch 2: 0.6249060034751892\n",
      "Loss in epoch 3: 0.6133942604064941\n",
      "Loss in epoch 4: 0.6053060293197632\n",
      "Loss in epoch 5: 0.599433422088623\n",
      "Loss in epoch 6: 0.5937463045120239\n",
      "Loss in epoch 7: 0.5896198153495789\n",
      "Loss in epoch 8: 0.585729718208313\n",
      "Loss in epoch 9: 0.581692099571228\n",
      "Loss in epoch 10: 0.5789421796798706\n",
      "Loss in epoch 11: 0.5760374069213867\n",
      "Loss in epoch 12: 0.5732020139694214\n",
      "Loss in epoch 13: 0.5714486837387085\n",
      "Loss in epoch 14: 0.5687288045883179\n",
      "Loss in epoch 15: 0.5671951770782471\n",
      "Loss in epoch 16: 0.5648314356803894\n",
      "Loss in epoch 17: 0.5630868673324585\n",
      "Loss in epoch 18: 0.5616310834884644\n",
      "Loss in epoch 19: 0.5600045919418335\n",
      "Loss in epoch 20: 0.5585857033729553\n",
      "Loss in epoch 21: 0.556941032409668\n",
      "Loss in epoch 22: 0.5562700629234314\n",
      "Loss in epoch 23: 0.5546447038650513\n",
      "Loss in epoch 24: 0.5535259246826172\n",
      "Loss in epoch 25: 0.551810085773468\n",
      "Loss in epoch 26: 0.5510674715042114\n",
      "Loss in epoch 27: 0.5500529408454895\n",
      "Loss in epoch 28: 0.5492801666259766\n",
      "Loss in epoch 29: 0.5478581786155701\n",
      "Loss in epoch 30: 0.5470687747001648\n",
      "Loss in epoch 31: 0.5460625886917114\n",
      "Loss in epoch 32: 0.5457226634025574\n",
      "Loss in epoch 33: 0.5442943572998047\n",
      "Loss in epoch 34: 0.543417751789093\n",
      "Loss in epoch 35: 0.5428622364997864\n",
      "Loss in epoch 36: 0.5419923067092896\n",
      "Loss in epoch 37: 0.5417805910110474\n",
      "Loss in epoch 38: 0.5408362746238708\n",
      "Loss in epoch 39: 0.5400168299674988\n",
      "Loss in epoch 40: 0.539539635181427\n",
      "Loss in epoch 41: 0.5393307209014893\n",
      "Loss in epoch 42: 0.5379636287689209\n",
      "Loss in epoch 43: 0.5375922918319702\n",
      "Loss in epoch 44: 0.537266731262207\n",
      "Loss in epoch 45: 0.5368777513504028\n",
      "Loss in epoch 46: 0.5361250638961792\n",
      "Loss in epoch 47: 0.5353794693946838\n",
      "Loss in epoch 48: 0.5353063941001892\n",
      "Loss in epoch 49: 0.5349287986755371\n",
      "Binary classification accuracy: 0.7402\n"
     ]
    }
   ],
   "source": [
    "# Ternary classification model\n",
    "ternary_model = SentimentClassifier(input_dim=300, output_dim=3) # for ternary classification\n",
    "ternary_optimizer = optim.Adam(ternary_model.parameters())\n",
    "\n",
    "train_model(ternary_model, optimizer=ternary_optimizer, train_data=pretrained_word_embedding, train_labels=train_labels_ternary, epochs=50)\n",
    "\n",
    "# Evaluate the ternary model\n",
    "accuracies[\"ter-pre-avg\"] = evaluate_model(ternary_model, test_data=pretrained_test_embedding, test_labels=test_labels_ternary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFNN on ternary data with my model's features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in epoch 0: 0.5888786911964417\n",
      "Loss in epoch 1: 0.5536072254180908\n",
      "Loss in epoch 2: 0.5439688563346863\n",
      "Loss in epoch 3: 0.5374993085861206\n",
      "Loss in epoch 4: 0.5318558812141418\n",
      "Loss in epoch 5: 0.5282536149024963\n",
      "Loss in epoch 6: 0.5247336030006409\n",
      "Loss in epoch 7: 0.5219700336456299\n",
      "Loss in epoch 8: 0.5197785496711731\n",
      "Loss in epoch 9: 0.5176863670349121\n",
      "Loss in epoch 10: 0.5158699154853821\n",
      "Loss in epoch 11: 0.5139257907867432\n",
      "Loss in epoch 12: 0.5123432874679565\n",
      "Loss in epoch 13: 0.5108899474143982\n",
      "Loss in epoch 14: 0.509605884552002\n",
      "Loss in epoch 15: 0.5083060264587402\n",
      "Loss in epoch 16: 0.5071828365325928\n",
      "Loss in epoch 17: 0.5058382749557495\n",
      "Loss in epoch 18: 0.5047503709793091\n",
      "Loss in epoch 19: 0.503792405128479\n",
      "Loss in epoch 20: 0.5029356479644775\n",
      "Loss in epoch 21: 0.5023841857910156\n",
      "Loss in epoch 22: 0.5011320114135742\n",
      "Loss in epoch 23: 0.5007970333099365\n",
      "Loss in epoch 24: 0.4996083676815033\n",
      "Loss in epoch 25: 0.49881136417388916\n",
      "Loss in epoch 26: 0.49829280376434326\n",
      "Loss in epoch 27: 0.4975651204586029\n",
      "Loss in epoch 28: 0.49710941314697266\n",
      "Loss in epoch 29: 0.4961783289909363\n",
      "Loss in epoch 30: 0.49570325016975403\n",
      "Loss in epoch 31: 0.49534502625465393\n",
      "Loss in epoch 32: 0.494770884513855\n",
      "Loss in epoch 33: 0.4943215250968933\n",
      "Loss in epoch 34: 0.4935002028942108\n",
      "Loss in epoch 35: 0.4930819272994995\n",
      "Loss in epoch 36: 0.4928090274333954\n",
      "Loss in epoch 37: 0.4921262562274933\n",
      "Loss in epoch 38: 0.4917065501213074\n",
      "Loss in epoch 39: 0.4912647604942322\n",
      "Loss in epoch 40: 0.4907691478729248\n",
      "Loss in epoch 41: 0.4905487596988678\n",
      "Loss in epoch 42: 0.4900917708873749\n",
      "Loss in epoch 43: 0.4898720681667328\n",
      "Loss in epoch 44: 0.4894718825817108\n",
      "Loss in epoch 45: 0.48897305130958557\n",
      "Loss in epoch 46: 0.4890151619911194\n",
      "Loss in epoch 47: 0.48831266164779663\n",
      "Loss in epoch 48: 0.48807692527770996\n",
      "Loss in epoch 49: 0.487934947013855\n",
      "Binary classification accuracy: 0.77184\n"
     ]
    }
   ],
   "source": [
    "# Ternary classification model\n",
    "ternary_model = SentimentClassifier(input_dim=300, output_dim=3) # for ternary classification\n",
    "ternary_optimizer = optim.Adam(ternary_model.parameters())\n",
    "\n",
    "train_model(ternary_model, optimizer=ternary_optimizer, train_data=my_word_embedding, train_labels=train_labels_ternary, epochs=50)\n",
    "\n",
    "# Evaluate the ternary model\n",
    "accuracies[\"ter-my-avg\"] = evaluate_model(ternary_model, test_data=my_test_embedding, test_labels=test_labels_ternary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 part b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet defines a class `MLP` for a multi-layer perceptron using PyTorch's `nn.Module`. The MLP architecture is specified during initialization with an input dimension `input_dim` and an output dimension `output_dim`, along with two hidden layers of sizes 50 and 10. The model dynamically constructs these layers, starting with a flattening layer to ensure input tensors are correctly shaped for linear layers. Each hidden layer is followed by a ReLU activation function for non-linearity. The final layer is a linear layer that maps to the output dimension. The `forward` method defines the data flow through the model, making it ready for training and inference with specified input and output dimensions. Here's how it's structured:\n",
    "\n",
    "1. **Initialization (`__init__` method)**: The constructor takes two arguments: `input_dim` for the size of the input layer and `output_dim` for the size of the output layer. It defines a network architecture with two hidden layers, specified by the `hidden_sizes` list containing 50 and 10 neurons, respectively.\n",
    "\n",
    "2. **Layer Construction**: \n",
    "   - Starts with a `nn.Flatten()` layer to ensure input tensors are flattened (useful if the input comes from previous layers that do not output flat vectors).\n",
    "   - Iteratively adds pairs of `nn.Linear` and `nn.ReLU` layers to the network based on `hidden_sizes`. The first `nn.Linear` layer's input size is `input_dim`, and subsequent layers use the previous layer's size. Each `nn.Linear` layer is followed by a `nn.ReLU` activation function for non-linearity.\n",
    "   - Concludes with an `nn.Linear` layer that maps from the last hidden layer to the output layer, sized according to `output_dim`.\n",
    "\n",
    "3. **Sequential Container**: The layers are wrapped in an `nn.Sequential` container, which automates the forward pass in the order the layers were added.\n",
    "\n",
    "4. **Forward Pass (`forward` method)**: Defines the forward propagation through the network. Given an input `x`, it passes through the sequential container (`self.model`) and returns the output. This method is automatically called by PyTorch during training and prediction.\n",
    "\n",
    "This class provides a flexible architecture for MLPs by allowing customization of input and output dimensions and the size of hidden layers, making it adaptable for various classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        hidden_sizes=[50,10]\n",
    "        layers = []\n",
    "        layers.append(nn.Flatten())\n",
    "        for i in range(len(hidden_sizes)):\n",
    "            layers.append(nn.Linear(input_dim if i == 0 else hidden_sizes[i-1], hidden_sizes[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], output_dim))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `concatenated_vectors` function processes a given review text to create a fixed-length vector representation using word vectors from a pretrained model. Here's the breakdown:\n",
    "\n",
    "- **Word Vector Retrieval**: It splits the review into words and fetches their corresponding vectors from the `model`, provided the words exist in the model's vocabulary.\n",
    "\n",
    "- **Zero Vector for Missing Words**: If the review doesn't contain any words found in the model, a zero vector of length `300 * max_words` is returned, assuming each word vector is 300-dimensional.\n",
    "\n",
    "- **Limiting Word Vectors**: The function limits the number of word vectors to `max_words`, truncating longer sequences.\n",
    "\n",
    "- **Padding Short Sequences**: If the number of word vectors is less than `max_words`, the function pads the sequence with zero vectors to reach the `max_words` length, ensuring uniform vector size.\n",
    "\n",
    "- **Flattening**: The sequence of vectors (either truncated or padded) is flattened into a single vector and returned.\n",
    "\n",
    "This approach allows for consistency in input vector size across varying lengths of review texts, making it suitable for machine learning models that require fixed-size inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenated_vectors(review, model, max_words=10):\n",
    "    words_vector = [model[word] for word in review.split() if word in model]\n",
    "    if len(words_vector) == 0:\n",
    "        return np.zeros(300 * max_words)\n",
    "    \n",
    "    words_vector = np.array(words_vector[:max_words])\n",
    "    padding_size = max_words - words_vector.shape[0]\n",
    "\n",
    "    if padding_size > 0:\n",
    "        padding = np.zeros((padding_size, words_vector.shape[1]))\n",
    "        words_vector = np.concatenate([words_vector, padding])\n",
    "\n",
    "    return words_vector.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "del my_word_embedding, my_test_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "del binary_model, ternary_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_word_embedding = np.asarray([concatenated_vectors(review=_review, model=wv, max_words=10) for _review in train_data_binary.review_lemmatize])\n",
    "pretrained_test_embedding = np.asarray([concatenated_vectors(review=review, model=wv) for review in test_data_binary.review_lemmatize])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained model with binary classes and concatenated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in epoch 0: 0.4530833065509796\n",
      "Loss in epoch 1: 0.39451682567596436\n",
      "Loss in epoch 2: 0.35656094551086426\n",
      "Loss in epoch 3: 0.3198108673095703\n",
      "Loss in epoch 4: 0.28353092074394226\n",
      "Loss in epoch 5: 0.24896378815174103\n",
      "Loss in epoch 6: 0.21619237959384918\n",
      "Loss in epoch 7: 0.18852007389068604\n",
      "Loss in epoch 8: 0.16450753808021545\n",
      "Loss in epoch 9: 0.14435173571109772\n",
      "Loss in epoch 10: 0.12878350913524628\n",
      "Loss in epoch 11: 0.11453308165073395\n",
      "Loss in epoch 12: 0.10356079041957855\n",
      "Loss in epoch 13: 0.0942152813076973\n",
      "Loss in epoch 14: 0.08782602101564407\n",
      "Loss in epoch 15: 0.08074415475130081\n",
      "Loss in epoch 16: 0.0753210186958313\n",
      "Loss in epoch 17: 0.07034900784492493\n",
      "Loss in epoch 18: 0.0673942118883133\n",
      "Loss in epoch 19: 0.06359957158565521\n",
      "Loss in epoch 20: 0.05768275260925293\n",
      "Loss in epoch 21: 0.05741740018129349\n",
      "Loss in epoch 22: 0.055370036512613297\n",
      "Loss in epoch 23: 0.05085262656211853\n",
      "Loss in epoch 24: 0.04838799685239792\n",
      "Binary classification accuracy: 0.776125\n"
     ]
    }
   ],
   "source": [
    "# Binary classification model\n",
    "binary_model = MLP(input_dim=3000, output_dim=2) # for binary classification\n",
    "binary_optimizer = optim.Adam(binary_model.parameters())\n",
    "\n",
    "train_model(binary_model, optimizer=binary_optimizer, train_data=pretrained_word_embedding, train_labels=train_labels_binary, epochs=25)\n",
    "\n",
    "# Evaluate the binary model\n",
    "accuracies[\"bin-pre-con\"] = evaluate_model(binary_model, test_data=pretrained_test_embedding, test_labels=test_labels_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My model with binary classes and concatenated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_word_embedding = np.asarray([concatenated_vectors(review=_review, model=my_model.wv, max_words=10) for _review in train_data_binary.review_lemmatize])\n",
    "my_test_embedding = np.asarray([concatenated_vectors(review=review, model=my_model.wv) for review in test_data_binary.review_lemmatize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = Word2Vec(sentences=train_data_binary.review_lemmatize.apply(lambda x: simple_preprocess(x)), vector_size=300, window=11, min_count=10, workers=4, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in epoch 0: 0.3980901837348938\n",
      "Loss in epoch 1: 0.35928189754486084\n",
      "Loss in epoch 2: 0.33583956956863403\n",
      "Loss in epoch 3: 0.30978691577911377\n",
      "Loss in epoch 4: 0.28075456619262695\n",
      "Loss in epoch 5: 0.24907425045967102\n",
      "Loss in epoch 6: 0.21675018966197968\n",
      "Loss in epoch 7: 0.18540360033512115\n",
      "Loss in epoch 8: 0.15746156871318817\n",
      "Loss in epoch 9: 0.1304992139339447\n",
      "Loss in epoch 10: 0.11095687001943588\n",
      "Loss in epoch 11: 0.0957595556974411\n",
      "Loss in epoch 12: 0.08113052695989609\n",
      "Loss in epoch 13: 0.07158970087766647\n",
      "Loss in epoch 14: 0.0649472251534462\n",
      "Loss in epoch 15: 0.056296128779649734\n",
      "Loss in epoch 16: 0.05383606627583504\n",
      "Loss in epoch 17: 0.0497620552778244\n",
      "Loss in epoch 18: 0.045983415096998215\n",
      "Loss in epoch 19: 0.04342653602361679\n",
      "Loss in epoch 20: 0.03997180983424187\n",
      "Loss in epoch 21: 0.03845454379916191\n",
      "Loss in epoch 22: 0.03768574073910713\n",
      "Loss in epoch 23: 0.03504708781838417\n",
      "Loss in epoch 24: 0.0341111496090889\n",
      "Binary classification accuracy: 0.79065\n"
     ]
    }
   ],
   "source": [
    "# Binary classification model\n",
    "binary_model = MLP(input_dim=3000, output_dim=2) # for binary classification\n",
    "binary_optimizer = optim.Adam(binary_model.parameters())\n",
    "\n",
    "train_model(binary_model, optimizer=binary_optimizer, train_data=my_word_embedding, train_labels=train_labels_binary, epochs=25)\n",
    "\n",
    "# Evaluate the binary model\n",
    "accuracies[\"bin-my-con\"] = evaluate_model(binary_model, test_data=my_test_embedding, test_labels=test_labels_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained model with ternary classes and concatenated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_word_embedding = np.asarray([concatenated_vectors(review=_review, model=wv, max_words=10) for _review in train_data_ternary.review_lemmatize])\n",
    "pretrained_test_embedding = np.asarray([concatenated_vectors(review=review, model=wv) for review in test_data_ternary.review_lemmatize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in epoch 0: 0.8186842203140259\n",
      "Loss in epoch 1: 0.7316397428512573\n",
      "Loss in epoch 2: 0.6879225969314575\n",
      "Loss in epoch 3: 0.6512019038200378\n",
      "Loss in epoch 4: 0.6178866624832153\n",
      "Loss in epoch 5: 0.5874897837638855\n",
      "Loss in epoch 6: 0.5589696764945984\n",
      "Loss in epoch 7: 0.5338996648788452\n",
      "Loss in epoch 8: 0.5116013288497925\n",
      "Loss in epoch 9: 0.49115726351737976\n",
      "Loss in epoch 10: 0.4721020758152008\n",
      "Loss in epoch 11: 0.45491644740104675\n",
      "Loss in epoch 12: 0.4395006597042084\n",
      "Loss in epoch 13: 0.42477601766586304\n",
      "Loss in epoch 14: 0.41179630160331726\n",
      "Loss in epoch 15: 0.3995373547077179\n",
      "Loss in epoch 16: 0.38791102170944214\n",
      "Loss in epoch 17: 0.3765275478363037\n",
      "Loss in epoch 18: 0.3660672605037689\n",
      "Loss in epoch 19: 0.35694098472595215\n",
      "Loss in epoch 20: 0.34789901971817017\n",
      "Loss in epoch 21: 0.3394908308982849\n",
      "Loss in epoch 22: 0.33109062910079956\n",
      "Loss in epoch 23: 0.3247874677181244\n",
      "Loss in epoch 24: 0.316448450088501\n",
      "Loss in epoch 25: 0.3099673092365265\n",
      "Loss in epoch 26: 0.30317214131355286\n",
      "Loss in epoch 27: 0.29707276821136475\n",
      "Loss in epoch 28: 0.29186272621154785\n",
      "Loss in epoch 29: 0.28602203726768494\n",
      "Loss in epoch 30: 0.2802155613899231\n",
      "Loss in epoch 31: 0.27620816230773926\n",
      "Loss in epoch 32: 0.2699589431285858\n",
      "Loss in epoch 33: 0.26563653349876404\n",
      "Loss in epoch 34: 0.2625548243522644\n",
      "Loss in epoch 35: 0.2570060193538666\n",
      "Loss in epoch 36: 0.2539196312427521\n",
      "Loss in epoch 37: 0.24942509829998016\n",
      "Loss in epoch 38: 0.24442456662654877\n",
      "Loss in epoch 39: 0.24308723211288452\n",
      "Loss in epoch 40: 0.2382010519504547\n",
      "Loss in epoch 41: 0.23469796776771545\n",
      "Loss in epoch 42: 0.23114009201526642\n",
      "Loss in epoch 43: 0.228440523147583\n",
      "Loss in epoch 44: 0.22522017359733582\n",
      "Loss in epoch 45: 0.2211892455816269\n",
      "Loss in epoch 46: 0.21972209215164185\n",
      "Loss in epoch 47: 0.21694207191467285\n",
      "Loss in epoch 48: 0.21572627127170563\n",
      "Loss in epoch 49: 0.21131925284862518\n",
      "Binary classification accuracy: 0.6121\n"
     ]
    }
   ],
   "source": [
    "# Ternary classification model\n",
    "ternary_model = MLP(input_dim=3000, output_dim=3) # for ternary classification\n",
    "ternary_optimizer = optim.Adam(ternary_model.parameters())\n",
    "\n",
    "train_model(ternary_model, optimizer=ternary_optimizer, train_data=pretrained_word_embedding, train_labels=train_labels_ternary, epochs=50)\n",
    "\n",
    "# Evaluate the ternary model\n",
    "accuracies[\"ter-pre-con\"] = evaluate_model(ternary_model, test_data=pretrained_test_embedding, test_labels=test_labels_ternary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My model with ternary classes and concatenated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_word_embedding = np.asarray([concatenated_vectors(review=_review, model=my_model.wv, max_words=10) for _review in train_data_ternary.review_lemmatize])\n",
    "my_test_embedding = np.asarray([concatenated_vectors(review=review, model=my_model.wv) for review in test_data_ternary.review_lemmatize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in epoch 0: 0.7359956502914429\n",
      "Loss in epoch 1: 0.6848767995834351\n",
      "Loss in epoch 2: 0.6637907028198242\n",
      "Loss in epoch 3: 0.6429949998855591\n",
      "Loss in epoch 4: 0.6211372017860413\n",
      "Loss in epoch 5: 0.5977684855461121\n",
      "Loss in epoch 6: 0.5749894380569458\n",
      "Loss in epoch 7: 0.5524516105651855\n",
      "Loss in epoch 8: 0.5303213000297546\n",
      "Loss in epoch 9: 0.5105023384094238\n",
      "Loss in epoch 10: 0.4917812943458557\n",
      "Loss in epoch 11: 0.4754505455493927\n",
      "Loss in epoch 12: 0.4601830840110779\n",
      "Loss in epoch 13: 0.44584763050079346\n",
      "Loss in epoch 14: 0.4333261251449585\n",
      "Loss in epoch 15: 0.42097917199134827\n",
      "Loss in epoch 16: 0.4098316431045532\n",
      "Loss in epoch 17: 0.39957061409950256\n",
      "Loss in epoch 18: 0.3896203935146332\n",
      "Loss in epoch 19: 0.3804463744163513\n",
      "Loss in epoch 20: 0.37094733119010925\n",
      "Loss in epoch 21: 0.36268481612205505\n",
      "Loss in epoch 22: 0.3556191921234131\n",
      "Loss in epoch 23: 0.3474865257740021\n",
      "Loss in epoch 24: 0.3405013978481293\n",
      "Loss in epoch 25: 0.335632860660553\n",
      "Loss in epoch 26: 0.32744455337524414\n",
      "Loss in epoch 27: 0.3223755359649658\n",
      "Loss in epoch 28: 0.31546977162361145\n",
      "Loss in epoch 29: 0.311544269323349\n",
      "Loss in epoch 30: 0.30449894070625305\n",
      "Loss in epoch 31: 0.302324116230011\n",
      "Loss in epoch 32: 0.29582780599594116\n",
      "Loss in epoch 33: 0.29131823778152466\n",
      "Loss in epoch 34: 0.2882073223590851\n",
      "Loss in epoch 35: 0.2832963764667511\n",
      "Loss in epoch 36: 0.2795276939868927\n",
      "Loss in epoch 37: 0.27543094754219055\n",
      "Loss in epoch 38: 0.27042996883392334\n",
      "Loss in epoch 39: 0.26732465624809265\n",
      "Loss in epoch 40: 0.2645481526851654\n",
      "Loss in epoch 41: 0.26050108671188354\n",
      "Loss in epoch 42: 0.2577091157436371\n",
      "Loss in epoch 43: 0.25496429204940796\n",
      "Loss in epoch 44: 0.25123336911201477\n",
      "Loss in epoch 45: 0.24801231920719147\n",
      "Loss in epoch 46: 0.24535462260246277\n",
      "Loss in epoch 47: 0.24216626584529877\n",
      "Loss in epoch 48: 0.23957130312919617\n",
      "Loss in epoch 49: 0.2365877479314804\n",
      "Binary classification accuracy: 0.62816\n"
     ]
    }
   ],
   "source": [
    "# Ternary classification model\n",
    "ternary_model = MLP(input_dim=3000, output_dim=3) # for ternary classification\n",
    "ternary_optimizer = optim.Adam(ternary_model.parameters())\n",
    "\n",
    "train_model(ternary_model, optimizer=ternary_optimizer, train_data=my_word_embedding, train_labels=train_labels_ternary, epochs=50)\n",
    "\n",
    "# Evaluate the ternary model\n",
    "accuracies[\"ter-my-con\"] = evaluate_model(ternary_model, test_data=my_test_embedding, test_labels=test_labels_ternary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pretrained_word_embedding, pretrained_test_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do you conclude by comparing accuracy values you obtain with those obtained in the \"Simple Models\" section?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `create_word_vector` function processes a single review text to generate a fixed-size word vector representation using a pretrained Word2Vec model. It performs the following steps:\n",
    "\n",
    "- It tokenizes the review into individual words and retrieves their corresponding vectors from the Word2Vec model, ensuring only words present in the model are considered.\n",
    "- If no words from the review are found in the model, a tensor of zeros with dimensions `[max_length, vector_size]` is returned to represent an empty word vector.\n",
    "- If there are word vectors, they are truncated or padded with zeros to ensure the output tensor has a uniform shape of `[max_length, vector_size]`, where `max_length` is the specified maximum number of words and `vector_size` is the dimensionality of each word vector.\n",
    "- The function returns this tensor, which can be directly used for model input, ensuring consistency in input size regardless of the original review length.-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_vector(review, model, max_length, vector_size):\n",
    "    # Load and preprocess data for a single sample\n",
    "    words_vector = [model[word] for word in review.split() if word in model]\n",
    "    if len(words_vector) == 0:\n",
    "        words_vector = torch.zeros(max_length, vector_size)\n",
    "    else:\n",
    "        words_vector = torch.tensor(words_vector[:max_length])\n",
    "\n",
    "    # Pad or truncate the sequence\n",
    "    if words_vector.shape[0] < max_length:\n",
    "        padding_size = max_length - words_vector.shape[0]\n",
    "        padding = torch.zeros(padding_size, vector_size)\n",
    "        words_vector = torch.cat([words_vector, padding])\n",
    "\n",
    "    return words_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "del final_copy, final_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `run_cnn` function orchestrates the training and evaluation of a Convolutional Neural Network (CNN) for text classification tasks using PyTorch. It accepts both training and testing datasets along with their labels, parameters defining the CNN architecture, a Word2Vec model for vectorizing text, and other training parameters. Here's a detailed overview:\n",
    "\n",
    "### Parameters:\n",
    "- `train_data`, `test_data`: Arrays or lists containing the raw text reviews for training and testing, respectively.\n",
    "- `train_labels`, `test_labels`: The corresponding labels for the training and testing datasets.\n",
    "- `vector_size`: The dimensionality of the Word2Vec vectors.\n",
    "- `op_channel_s1`, `op_channel_s2`: The number of output channels for the first and second convolutional layers, respectively.\n",
    "- `num_classes`: The number of classes for classification.\n",
    "- `word2vec`: A pretrained Word2Vec model used to vectorize the text data.\n",
    "- `max_length`: The maximum number of words considered from each review for creating fixed-length input vectors (default is 50).\n",
    "- `batch_size`: The size of each batch used during training (default is 32).\n",
    "- `epochs`: The number of training iterations over the entire dataset (default is 10).\n",
    "- `learning_rate`: The learning rate for the optimizer (default is 0.001).\n",
    "\n",
    "### Process:\n",
    "1. **Data Preparation**: It vectorizes the text data from `train_data` and `test_data` using the `create_word_vector` function, considering only the first `max_length` words from each review and setting the vector size as per the `word2vec` model. The vectorized data is then converted to PyTorch tensors.\n",
    "\n",
    "2. **DataLoader Creation**: The tensors are wrapped in a `TensorDataset` and loaded using a `DataLoader`, facilitating efficient batch processing during training and testing.\n",
    "\n",
    "3. **CNN Model Definition**: Defines a sequential CNN model with two convolutional layers followed by ReLU activations, a flattening layer, and a final linear layer for classification. The CNN expects input with dimensions `[batch_size, vector_size, max_length]`.\n",
    "\n",
    "4. **Training**: In the training loop, the model is trained on the `train_loader` dataset using the Adam optimizer and CrossEntropyLoss. The inputs are permuted to match the CNN's expected input dimensions, and the average loss per epoch is printed.\n",
    "\n",
    "5. **Evaluation**: The function evaluates the model's accuracy on the `test_loader` dataset, calculating the proportion of correctly predicted labels.\n",
    "\n",
    "### Returns:\n",
    "- `testing_accuracy`: The accuracy of the model on the test dataset, calculated as the number of correct predictions divided by the total number of predictions.\n",
    "\n",
    "This function provides a comprehensive workflow for training and testing a CNN model for text classification, encapsulating data preprocessing, model training, and evaluation within a single, convenient function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cnn(train_data, train_labels, test_data, test_labels, vector_size, op_channel_s1, op_channel_s2, num_classes, word2vec, max_length=50, batch_size=32, epochs=10, learning_rate=0.001):\n",
    "    train_data_limit = np.asarray([create_word_vector(review=str(review), model=word2vec, max_length=50, vector_size=300 ) for review in train_data])\n",
    "    test_data_limit = np.asarray([create_word_vector(review=str(review), model=word2vec, max_length=50, vector_size=300 ) for review in test_data])\n",
    "    \n",
    "    train_data_tensor = torch.from_numpy(train_data_limit).to(dtype=torch.float32)\n",
    "    train_labels_tensor = torch.tensor(train_labels.values, dtype=torch.long)\n",
    "    train_dataset = TensorDataset(train_data_tensor, train_labels_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    test_data_tensor = torch.from_numpy(test_data_limit).to(dtype=torch.float32)\n",
    "    test_labels_tensor = torch.tensor(test_labels.values, dtype=torch.long)\n",
    "    test_dataset = TensorDataset(test_data_tensor, test_labels_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Define the CNN model using Sequential\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv1d(in_channels=vector_size, out_channels=op_channel_s1, kernel_size=3),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv1d(in_channels=op_channel_s1, out_channels=op_channel_s2, kernel_size=3),\n",
    "        nn.ReLU(),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(op_channel_s2 * (max_length - 4), num_classes)\n",
    "    )\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    print(f'\\nTraining Loss for each epoch :')\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        losses = []\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            # No need to check for sparse input, as Conv1d layer expects dense input\n",
    "            outputs = model(inputs.permute(0, 2, 1))\n",
    "            loss = criterion(outputs, labels)\n",
    "            losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Average loss for the epoch\n",
    "        print(f'Loss in epoch {epoch}: {sum(losses)/len(losses)}')\n",
    "        \n",
    "    # Testing loop\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs.permute(0, 2, 1))\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    testing_accuracy = correct / total\n",
    "    return testing_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "del my_word_embedding, my_test_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### binary data on my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_word_embedding= averaged_word_vectorizer(train_data_binary['review_lemmatize'], my_model.wv, 300)\n",
    "my_test_embedding = averaged_word_vectorizer(test_data_binary['review_lemmatize'], my_model.wv, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jc/l_cy7_tj5pn7bp93mtmcrhg80000gn/T/ipykernel_11120/2942501205.py:7: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  words_vector = torch.tensor(words_vector[:max_length])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss for each epoch :\n",
      "Loss in epoch 0: 0.2515266239643097\n",
      "Loss in epoch 1: 0.20382119715213776\n",
      "Loss in epoch 2: 0.1852751523256302\n",
      "Loss in epoch 3: 0.16978009045124054\n",
      "Loss in epoch 4: 0.15757496654987335\n",
      "Loss in epoch 5: 0.14520657062530518\n",
      "Loss in epoch 6: 0.13318829238414764\n",
      "Loss in epoch 7: 0.1235203742980957\n",
      "Loss in epoch 8: 0.11423000693321228\n",
      "Loss in epoch 9: 0.10546647757291794\n",
      "Binary Test Accuracy: 0.9125\n"
     ]
    }
   ],
   "source": [
    "vector_size = 300\n",
    "op_channel_s1 = 50\n",
    "op_channel_s2 = 10\n",
    "output_size = 2\n",
    "\n",
    "accuracy_binary = run_cnn(\n",
    "    train_data_binary.review_lemmatize, train_labels_binary, test_data_binary.review_lemmatize, test_labels_binary, vector_size, op_channel_s1, op_channel_s2, output_size, my_model.wv,\n",
    ")\n",
    "\n",
    "print(f'Binary Test Accuracy: {accuracy_binary}')\n",
    "accuracies['bin-my-avg'] = accuracy_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ternary data on my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_word_embedding= averaged_word_vectorizer(train_data_ternary['review_lemmatize'], my_model.wv, 300)\n",
    "my_test_embedding = averaged_word_vectorizer(test_data_ternary['review_lemmatize'], my_model.wv, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss for each epoch :\n",
      "Loss in epoch 0: 0.5690693259239197\n",
      "Loss in epoch 1: 0.5156851410865784\n",
      "Loss in epoch 2: 0.4962995648384094\n",
      "Loss in epoch 3: 0.4820621907711029\n",
      "Loss in epoch 4: 0.4688960611820221\n",
      "Loss in epoch 5: 0.4572489559650421\n",
      "Loss in epoch 6: 0.4471859633922577\n",
      "Loss in epoch 7: 0.43758049607276917\n",
      "Loss in epoch 8: 0.4283675253391266\n",
      "Loss in epoch 9: 0.4206487834453583\n",
      "Ternary Test Accuracy: 0.7782\n"
     ]
    }
   ],
   "source": [
    "vector_size = 300\n",
    "op_channel_s1 = 50\n",
    "op_channel_s2 = 10\n",
    "output_size = 3\n",
    "\n",
    "accuracy_ternary = run_cnn(\n",
    "    train_data_ternary.review_lemmatize, train_labels_ternary, test_data_ternary.review_lemmatize, test_labels_ternary, vector_size, op_channel_s1, op_channel_s2, output_size, my_model.wv,\n",
    ")\n",
    "\n",
    "print(f'Ternary Test Accuracy: {accuracy_ternary}')\n",
    "accuracies['ter-my-avg'] = accuracy_ternary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### binary data on pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_word_embedding = np.asarray([concatenated_vectors(review=_review, model=my_model.wv, max_words=10) for _review in train_data_binary.review_lemmatize])\n",
    "my_test_embedding = np.asarray([concatenated_vectors(review=review, model=my_model.wv) for review in test_data_binary.review_lemmatize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss for each epoch :\n",
      "Loss in epoch 0: 0.2650148868560791\n",
      "Loss in epoch 1: 0.20370304584503174\n",
      "Loss in epoch 2: 0.18364916741847992\n",
      "Loss in epoch 3: 0.1674783080816269\n",
      "Loss in epoch 4: 0.15297238528728485\n",
      "Loss in epoch 5: 0.14112749695777893\n",
      "Loss in epoch 6: 0.1287354677915573\n",
      "Loss in epoch 7: 0.11844947189092636\n",
      "Loss in epoch 8: 0.1087835431098938\n",
      "Loss in epoch 9: 0.10171432793140411\n",
      "Binary Test Accuracy: 0.904925\n"
     ]
    }
   ],
   "source": [
    "vector_size = 300\n",
    "op_channel_s1 = 50\n",
    "op_channel_s2 = 10\n",
    "output_size = 2\n",
    "\n",
    "accuracy_binary = run_cnn(\n",
    "    train_data_binary.review_lemmatize, train_labels_binary, test_data_binary.review_lemmatize, test_labels_binary, vector_size, op_channel_s1, op_channel_s2, output_size, wv,\n",
    ")\n",
    "\n",
    "print(f'Binary Test Accuracy: {accuracy_binary}')\n",
    "accuracies['bin-my-con'] = accuracy_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ternary data on pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_word_embedding = np.asarray([concatenated_vectors(review=_review, model=my_model.wv, max_words=10) for _review in train_data_ternary.review_lemmatize])\n",
    "my_test_embedding = np.asarray([concatenated_vectors(review=review, model=my_model.wv) for review in test_data_ternary.review_lemmatize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss for each epoch :\n",
      "Loss in epoch 0: 0.6057630181312561\n",
      "Loss in epoch 1: 0.5228792428970337\n",
      "Loss in epoch 2: 0.5015048384666443\n",
      "Loss in epoch 3: 0.48573336005210876\n",
      "Loss in epoch 4: 0.473037451505661\n",
      "Loss in epoch 5: 0.4609887897968292\n",
      "Loss in epoch 6: 0.45106977224349976\n",
      "Loss in epoch 7: 0.4418560266494751\n",
      "Loss in epoch 8: 0.433174192905426\n",
      "Loss in epoch 9: 0.4255262017250061\n",
      "Ternary Test Accuracy: 0.76922\n"
     ]
    }
   ],
   "source": [
    "vector_size = 300\n",
    "op_channel_s1 = 50\n",
    "op_channel_s2 = 10\n",
    "output_size = 3\n",
    "\n",
    "accuracy_ternary = run_cnn(\n",
    "    train_data_ternary.review_lemmatize, train_labels_ternary, test_data_ternary.review_lemmatize, test_labels_ternary, vector_size, op_channel_s1, op_channel_s2, output_size, wv,\n",
    ")\n",
    "\n",
    "print(f'Ternary Test Accuracy: {accuracy_ternary}')\n",
    "accuracies['ter-my-con'] = accuracy_ternary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Classes | Model | Vector Type | Accuracy|\n",
    "|----------------|-------|-------------|---------|\n",
    "|Binary|Pereptron|Word2Vec - Pretrained model|81.39%|\n",
    "|Binary|Pereptron|Word2Vec - My model|82.18%|\n",
    "|Binary|Pereptron|TF-IDF|91.33%|\n",
    "|Binary|Pereptron|Word2Vec - My model|82.18%|\n",
    "|Binary|SVM|Word2Vec - Pretained model|86.23%|\n",
    "|Binary|SVM|Word2Vec - My model|89.88%|\n",
    "|Binary|SVM|Word2Vec - My model|82.18%|\n",
    "|Binary|SVM|TF-IDF|92.95%|\n",
    "|Binary|SVM|Word2Vec - My model|82.18%|\n",
    "|Binary|FFNN|Word2Vec - Pretrained model (average vector function)|88.64%|\n",
    "|Binary|FFNN|Word2Vec - My model (average vector function)|90.72%|\n",
    "|Ternary|FFNN|Word2Vec - Pretrained model (average vector function)|74.02%|\n",
    "|Ternary|FFNN|Word2Vec - My model (average vector function)|77.18%|\n",
    "|Binary|FFNN|Word2Vec - Pretrained model (concate vector function)|77.61%|\n",
    "|Binary|FFNN|Word2Vec - My model (concate vector function)|79.06%|\n",
    "|Ternary|FFNN|Word2Vec - Pretrained model (concate vector function)|61.21%|\n",
    "|Ternary|FFNN|Word2Vec - My model (concate vector function)|62.81%|\n",
    "|Binary|CNN|Word2Vec - My model (custom vector function)|91.25%|\n",
    "|Ternary|CNN|Word2Vec - My model (custom vector function)|77.82%|\n",
    "|Binary|CNN|Word2Vec - Pretrained model (custom vector function)|90.49%|\n",
    "|Ternary|CNN|Word2Vec - Pretrained model (custom vector function)|76.92%|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
